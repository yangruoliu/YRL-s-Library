[
  {
    "title": "Thyme: Think Beyond Images",
    "filename": "Thyme-Think_Beyond_Images.pdf",
    "year": 2025.8,
    "affiliations": ["Kwai Keye"],
    "github_repo": "https://github.com/yfzhang114/Thyme"
  },
  {
    "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency",
    "filename": "2508.18265v2.pdf",
    "year": 2025.8,
    "affiliations": ["InternVL Team, Shanghai AI Laboratory"],
    "github_repo": "https://github.com/OpenGVLab/InternVL",
    "arxiv": "https://arxiv.org/abs/2508.18265"
  },
  {
    "title": "GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning",
    "filename": "GLM-4.5v_and_GLM-4,1v-thinking.pdf",
    "year": 2025.7,
    "affiliations": ["Zhipu AI","Tsinghua University"],
    "github_repo": "https://github.com/zai-org/ GLM-V"
  },
  {
    "title": "MiMo-VL Technical Report",
    "filename": "2506.03569v1.pdf",
    "year": 2025.6,
    "affiliations": ["LLM-Core Xiaomi"],
    "github_repo": "https://github.com/XiaomiMiMo/MiMo-VL",
    "arxiv": "https://arxiv.org/abs/2506.03569"
  },
  {
    "title": "Show-o2: Improved Native Unified Multimodal Models",
    "filename": "2506.15564v2.pdf",
    "year": 2025.6,
    "affiliations": ["Show Lab, NUS"],
    "github_repo": "https://github.com/showlab/Show-o",
    "arxiv": "https://arxiv.org/abs/2506.15564"
  },
  {
    "title": "MMSearch-R1: Incentivizing LMMs to Search",
    "filename": "2506.20670v1.pdf",
    "year": 2025.6,
    "affiliations": ["ByteDance","S-Lab, NTU"],
    "github_repo": "https://github.com/EvolvingLMMs-Lab/multimodal-search-r1",
    "arxiv": "https://arxiv.org/abs/2506.20670"
  },
  {
    "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model",
    "filename": "2505.03739v1.pdf",
    "year": 2025.5,
    "affiliations": ["Tencent Youtu Lab"],
    "github_repo": "https://github.com/VITA-MLLM/VITA-Audio",
    "arxiv": "https://arxiv.org/abs/2505.03739"
  },
  {
    "title": "Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models",
    "filename": "2505.04921v2.pdf",
    "year": 2025.5,
    "affiliations": ["Harbin Institute of Technology, Shenzhen"],
    "github_repo": "https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models",
    "arxiv": "https://arxiv.org/abs/2505.04921",
    "remark": "综述"
  },
  {
    "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models—Architecture, Training and Dataset",
    "filename": "2505.09568v1.pdf",
    "year": 2025.5,
    "affiliations": ["Salesforce Research"],
    "github_repo": "https://github.com/JiuhaiChen/BLIP3o",
    "arxiv": "https://arxiv.org/abs/2505.09568"
  },
  {
    "title": "UniGen: Enhanced Training&Test-Time Strategies for Unified Multimodal Understanding and Generation",
    "filename": "2505.14682v1.pdf",
    "year": 2025.5,
    "affiliations": ["Apple"],
    "arxiv": "https://arxiv.org/abs/2505.14682"
  },
  {
    "title": "MMaDA: Multimodal Large Diffusion Language Models",
    "filename": "2505.15809v1.pdf",
    "year": 2025.5,
    "affiliations": ["Princeton University"],
    "github_repo": "https://github.com/Gen-Verse/MMaDA",
    "arxiv": "https://arxiv.org/abs/2505.15809",
    "remark": "ByteDance Seed"
  },
  {
    "title": "Emerging Properties in Unified Multimodal Pretraining",
    "filename": "2505.14683v3.pdf",
    "year": 2025.5,
    "affiliations": ["ByteDance Seed","Shenzhen Institutes of Advanced Technology"],
    "github_repo": "https://bagel-ai.org/",
    "arxiv": "https://arxiv.org/abs/2505.14683",
    "remark": "Bagel-AI"
  },
  {
    "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models",
    "filename": "2504.10479v4.pdf",
    "year": 2025.4,
    "affiliations": ["Shanghai AI Laboratory "],
    "github_repo": "https://github.com/OpenGVLab/InternVL",
    "arxiv": "https://arxiv.org/abs/2504.10479"
  },
  {
    "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning",
    "filename": "2504.16656v4.pdf",
    "year": 2025.4,
    "affiliations": ["Skywork AI"],
    "github_repo": "https://github.com/SkyworkAI/Skywork-R1V",
    "arxiv": "https://arxiv.org/abs/2504.16656"
  },
  {
    "title": "Qwen2.5-Omni Technical Report",
    "filename": "Qwen2.5_Omni.pdf",
    "year": 2025.3,
    "affiliations": ["Qwen Team"],
    "github_repo": "https://github.com/QwenLM/Qwen2.5-Omni"
  },
  {
    "title": "NEXUS: AN OMNI-PERCEPTIVE AND-INTERACTIVE MODEL FOR LANGUAGE, AUDIO, AND VISION",
    "filename": "2503.01879v3.pdf",
    "year": 2025.3,
    "affiliations": ["HiThink Research"],
    "github_repo": "https://hithink-ai.github.io/Nexus-O/",
    "arxiv": "https://arxiv.org/abs/2503.01879"
  },
  {
    "title": "Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuracy",
    "filename": "2502.05177v2.pdf",
    "year": 2025.2,
    "affiliations": ["Tencent Youtu Lab"],
    "github_repo": "https://github.com/VITA-MLLM/Long-VITA",
    "arxiv": "https://arxiv.org/abs/2502.05177"
  },
  {
    "title": "LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs",
    "filename": "2501.06186v1.pdf",
    "year": 2025.1,
    "venue": "ACL2025",
    "affiliations": ["Mohamed bin Zayed University of AI"],
    "github_repo": "https://github.com/mbzuai-oryx/LlamaV-o1",
    "arxiv": "https://arxiv.org/abs/2501.06186"
  },
  {
    "title": "InternVL2.5--Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
    "filename": "2412.05271v4.pdf",
    "year": 2024.12,
    "venue": "CVPR2024",
    "affiliations": ["Shanghai AI Laboratory"],
    "github_repo": "https://github.com/OpenGVLab/InternVL",
    "arxiv": "https://arxiv.org/abs/2412.05271"
  },
  {
    "title": "Apollo: An Exploration of Video Understanding in Large Multimodal Models",
    "filename": "2412.10360v1.pdf",
    "year": 2024.12,
    "affiliations": ["Meta"],
    "github_repo": "https://apollo-lmms.github.io",
    "arxiv": "https://arxiv.org/abs/2412.10360"
  }
]


